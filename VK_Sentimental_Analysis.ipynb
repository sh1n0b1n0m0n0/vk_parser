{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webapp import create_app\n",
    "from vk_parser import *\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pymorphy2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Обработка текста через регулярные выражения.\n",
    "- Удаление всех url из текста.\n",
    "- Удаление хэштегов, если они есть в тексте.\n",
    "- Удаление имён пользователей.\n",
    "- Удаление пунктуации и цифр.\n",
    "- Приведение всего текста к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower().replace('ё', 'е')\n",
    "    text = text.replace('d', '')\n",
    "    text = text.replace('rt', '')\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text)\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "    text = re.sub(r'[a-zA-Z]', '', text)\n",
    "    text = re.sub(r'\\b[a-zA-Zа-яА-Я1-9]\\b', '', text)\n",
    "    text = re.sub('\\b(?:[Хх][аоеи]+)+\\b', '', text)\n",
    "    text = re.sub(r'[0-9]', '', text)    \n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запрос к БД на извлечение текста комментарий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = create_app()\n",
    "with app.app_context():\n",
    "    query = Comment.query.all()\n",
    "    preprop_comm = []\n",
    "\n",
    "    for comment in query:\n",
    "        preprop_comm.append(preprocess_text(comment.comment_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_data = [preprocess_text(t) for t in preprop_comm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1220\n"
     ]
    }
   ],
   "source": [
    "print(len(bd_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_tokenized_commens = tokenaizer(bd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1220\n"
     ]
    }
   ],
   "source": [
    "print(len(bd_tokenized_commens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_prep_comments = []\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "for words in bd_tokenized_commens:\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            bd_prep_comments.append(word)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7215"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bd_prep_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "default_list = []\n",
    "for word in bd_prep_comments:\n",
    "    lem_word = morph.parse(str(word))[0].normal_form\n",
    "    default_list.append(lem_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7215"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(default_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считаем частоту слов в нашем предобработанном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "\n",
    "for token in default_list:\n",
    "    if token not in wordfreq.keys():\n",
    "        wordfreq[token] = 1\n",
    "    else:\n",
    "        wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(default_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ааа', 'ааааа', 'абаддон', ..., 'ярость', 'ясно', 'ёлочка'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_res = np.unique(res) \n",
    "unique_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3507"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_unique_words = unique_res.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Используем корпус Юлии Рубцовой русских твитов для тренировки модели.\n",
    "- Скачиваем корпус позитивных и негативных твитов с сайта http://study.mokoron.com/\n",
    "- Добавляем дополнительную колонку для позитивных (1) и негативных твитов (0), тем самым размечая данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем данные\n",
    "n = ['id', 'date', 'name', 'text', 'typr', 'rep', 'rtw', 'faw', 'stcount', 'foll', 'frien', 'listcount']\n",
    "data_positive = pd.read_csv('training_data/positive.csv', sep=';', error_bad_lines=False, names=n, usecols=['text'])\n",
    "data_negative = pd.read_csv('training_data/negative.csv', sep=';', error_bad_lines=False, names=n, usecols=['text'])\n",
    "\n",
    "# Формируем сбалансированный датасет\n",
    "sample_size = min(data_positive.shape[0], data_negative.shape[0])\n",
    "\n",
    "raw_data = np.concatenate((data_positive['text'].values[:sample_size],\n",
    "                           data_negative['text'].values[:sample_size]), axis=0)\n",
    "labels = [1] * sample_size + [0] * sample_size\n",
    "\n",
    "data = [preprocess_text(t) for t in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223846 223846 223846\n"
     ]
    }
   ],
   "source": [
    "print(len(labels), len(raw_data), len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Скачиваем набор русских стоп-слов для предобработки наших твитов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kamikaze666\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kamikaze666\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Разбиваем предобработанные слова на токены.\n",
    "- Удаляем стоп слова.\n",
    "- Производим лемматизацию или стемминг текста.\n",
    "- Удаляем короткие слова длинной меньше 2 символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit tokenized_words = [word_tokenize(sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenaizer(data):\n",
    "    tokens= [word_tokenize(sent) for sent in data]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = tokenaizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223846\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стемминг слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "def stemmer(word, stem_init):\n",
    "    return stem_init.stem(str(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "def delete_stop_words_with_stemming(words, stop_words):    \n",
    "    default_list = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            stemmed_word = stemmer(word, snowball)\n",
    "            default_list.append(stemmed_word)\n",
    "\n",
    "    return list(default_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_data_frame(data, labels):\n",
    "    default_list = []\n",
    "    \n",
    "    \n",
    "    for string in data:\n",
    "        new_string = ' '.join(string)\n",
    "        default_list.append(new_string)\n",
    "    \n",
    "    \n",
    "    default_list = pd.DataFrame(list(zip(default_list, labels)), columns =['comments', 'sentiment'])\n",
    "    return default_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 of 7\n",
      "Processed 2000 of 6\n",
      "Processed 3000 of 15\n",
      "Processed 4000 of 9\n",
      "Processed 5000 of 14\n",
      "Processed 6000 of 14\n",
      "Processed 7000 of 7\n",
      "Processed 8000 of 16\n",
      "Processed 9000 of 19\n",
      "Processed 10000 of 11\n",
      "Processed 11000 of 7\n",
      "Processed 12000 of 6\n",
      "Processed 13000 of 9\n",
      "Processed 14000 of 8\n",
      "Processed 15000 of 13\n",
      "Processed 16000 of 8\n",
      "Processed 17000 of 6\n",
      "Processed 18000 of 11\n",
      "Processed 19000 of 9\n",
      "Processed 20000 of 10\n",
      "Processed 21000 of 11\n",
      "Processed 22000 of 2\n",
      "Processed 23000 of 7\n",
      "Processed 24000 of 12\n",
      "Processed 25000 of 7\n",
      "Processed 26000 of 11\n",
      "Processed 27000 of 10\n",
      "Processed 28000 of 5\n",
      "Processed 29000 of 13\n",
      "Processed 30000 of 11\n",
      "Processed 31000 of 16\n",
      "Processed 32000 of 3\n",
      "Processed 33000 of 5\n",
      "Processed 34000 of 7\n",
      "Processed 35000 of 17\n",
      "Processed 36000 of 14\n",
      "Processed 37000 of 6\n",
      "Processed 38000 of 9\n",
      "Processed 39000 of 16\n",
      "Processed 40000 of 5\n",
      "Processed 41000 of 6\n",
      "Processed 42000 of 7\n",
      "Processed 43000 of 6\n",
      "Processed 44000 of 7\n",
      "Processed 45000 of 9\n",
      "Processed 46000 of 13\n",
      "Processed 47000 of 9\n",
      "Processed 48000 of 7\n",
      "Processed 49000 of 19\n",
      "Processed 50000 of 9\n",
      "Processed 51000 of 8\n",
      "Processed 52000 of 9\n",
      "Processed 53000 of 3\n",
      "Processed 54000 of 18\n",
      "Processed 55000 of 12\n",
      "Processed 56000 of 6\n",
      "Processed 57000 of 21\n",
      "Processed 58000 of 18\n",
      "Processed 59000 of 7\n",
      "Processed 60000 of 9\n",
      "Processed 61000 of 18\n",
      "Processed 62000 of 10\n",
      "Processed 63000 of 7\n",
      "Processed 64000 of 7\n",
      "Processed 65000 of 7\n",
      "Processed 66000 of 11\n",
      "Processed 67000 of 7\n",
      "Processed 68000 of 7\n",
      "Processed 69000 of 4\n",
      "Processed 70000 of 4\n",
      "Processed 71000 of 10\n",
      "Processed 72000 of 14\n",
      "Processed 73000 of 9\n",
      "Processed 74000 of 11\n",
      "Processed 75000 of 10\n",
      "Processed 76000 of 6\n",
      "Processed 77000 of 6\n",
      "Processed 78000 of 13\n",
      "Processed 79000 of 11\n",
      "Processed 80000 of 7\n",
      "Processed 81000 of 18\n",
      "Processed 82000 of 14\n",
      "Processed 83000 of 4\n",
      "Processed 84000 of 16\n",
      "Processed 85000 of 7\n",
      "Processed 86000 of 5\n",
      "Processed 87000 of 6\n",
      "Processed 88000 of 8\n",
      "Processed 89000 of 17\n",
      "Processed 90000 of 18\n",
      "Processed 91000 of 12\n",
      "Processed 92000 of 8\n",
      "Processed 93000 of 3\n",
      "Processed 94000 of 2\n",
      "Processed 95000 of 16\n",
      "Processed 96000 of 15\n",
      "Processed 97000 of 10\n",
      "Processed 98000 of 9\n",
      "Processed 99000 of 9\n",
      "Processed 100000 of 12\n",
      "Processed 101000 of 14\n",
      "Processed 102000 of 20\n",
      "Processed 103000 of 8\n",
      "Processed 104000 of 6\n",
      "Processed 105000 of 9\n",
      "Processed 106000 of 10\n",
      "Processed 107000 of 6\n",
      "Processed 108000 of 17\n",
      "Processed 109000 of 20\n",
      "Processed 110000 of 8\n",
      "Processed 111000 of 11\n",
      "Processed 112000 of 7\n",
      "Processed 113000 of 8\n",
      "Processed 114000 of 22\n",
      "Processed 115000 of 6\n",
      "Processed 116000 of 7\n",
      "Processed 117000 of 11\n",
      "Processed 118000 of 17\n",
      "Processed 119000 of 9\n",
      "Processed 120000 of 11\n",
      "Processed 121000 of 8\n",
      "Processed 122000 of 13\n",
      "Processed 123000 of 9\n",
      "Processed 124000 of 7\n",
      "Processed 125000 of 6\n",
      "Processed 126000 of 9\n",
      "Processed 127000 of 6\n",
      "Processed 128000 of 4\n",
      "Processed 129000 of 20\n",
      "Processed 130000 of 11\n",
      "Processed 131000 of 4\n",
      "Processed 132000 of 16\n",
      "Processed 133000 of 17\n",
      "Processed 134000 of 8\n",
      "Processed 135000 of 1\n",
      "Processed 136000 of 20\n",
      "Processed 137000 of 5\n",
      "Processed 138000 of 18\n",
      "Processed 139000 of 13\n",
      "Processed 140000 of 10\n",
      "Processed 141000 of 5\n",
      "Processed 142000 of 10\n",
      "Processed 143000 of 4\n",
      "Processed 144000 of 7\n",
      "Processed 145000 of 7\n",
      "Processed 146000 of 11\n",
      "Processed 147000 of 6\n",
      "Processed 148000 of 9\n",
      "Processed 149000 of 18\n",
      "Processed 150000 of 8\n",
      "Processed 151000 of 8\n",
      "Processed 152000 of 5\n",
      "Processed 153000 of 11\n",
      "Processed 154000 of 11\n",
      "Processed 155000 of 8\n",
      "Processed 156000 of 6\n",
      "Processed 157000 of 12\n",
      "Processed 158000 of 7\n",
      "Processed 159000 of 5\n",
      "Processed 160000 of 14\n",
      "Processed 161000 of 5\n",
      "Processed 162000 of 7\n",
      "Processed 163000 of 14\n",
      "Processed 164000 of 16\n",
      "Processed 165000 of 7\n",
      "Processed 166000 of 6\n",
      "Processed 167000 of 7\n",
      "Processed 168000 of 11\n",
      "Processed 169000 of 11\n",
      "Processed 170000 of 10\n",
      "Processed 171000 of 9\n",
      "Processed 172000 of 8\n",
      "Processed 173000 of 5\n",
      "Processed 174000 of 6\n",
      "Processed 175000 of 17\n",
      "Processed 176000 of 13\n",
      "Processed 177000 of 14\n",
      "Processed 178000 of 4\n",
      "Processed 179000 of 6\n",
      "Processed 180000 of 12\n",
      "Processed 181000 of 10\n",
      "Processed 182000 of 5\n",
      "Processed 183000 of 12\n",
      "Processed 184000 of 9\n",
      "Processed 185000 of 7\n",
      "Processed 186000 of 4\n",
      "Processed 187000 of 5\n",
      "Processed 188000 of 10\n",
      "Processed 189000 of 11\n",
      "Processed 190000 of 11\n",
      "Processed 191000 of 3\n",
      "Processed 192000 of 16\n",
      "Processed 193000 of 12\n",
      "Processed 194000 of 2\n",
      "Processed 195000 of 11\n",
      "Processed 196000 of 14\n",
      "Processed 197000 of 14\n",
      "Processed 198000 of 3\n",
      "Processed 199000 of 7\n",
      "Processed 200000 of 17\n",
      "Processed 201000 of 14\n",
      "Processed 202000 of 8\n",
      "Processed 203000 of 13\n",
      "Processed 204000 of 12\n",
      "Processed 205000 of 17\n",
      "Processed 206000 of 10\n",
      "Processed 207000 of 10\n",
      "Processed 208000 of 7\n",
      "Processed 209000 of 5\n",
      "Processed 210000 of 19\n",
      "Processed 211000 of 9\n",
      "Processed 212000 of 7\n",
      "Processed 213000 of 16\n",
      "Processed 214000 of 9\n",
      "Processed 215000 of 13\n",
      "Processed 216000 of 15\n",
      "Processed 217000 of 6\n",
      "Processed 218000 of 10\n",
      "Processed 219000 of 8\n",
      "Processed 220000 of 6\n",
      "Processed 221000 of 12\n",
      "Processed 222000 of 5\n",
      "Processed 223000 of 5\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = []\n",
    "counter = 0\n",
    "for part_of_list in tokenized_words:\n",
    "    stemmed_words.append(delete_stop_words_with_stemming(part_of_list, stop_words))\n",
    "    counter += 1\n",
    "    if counter%1000 == 0:\n",
    "        print(\"Processed {} of {}\".format(counter, len(part_of_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = to_data_frame(stemmed_words, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Экземпляры класса MorphAnalyzer обычно занимают порядка 15Мб оперативной памяти (т.к. загружают в память словари, данные для предсказателя и т.д.); старайтесь организовать свой код так, чтобы создавать экземпляр MorphAnalyzer заранее и работать с этим единственным экземпляром в дальнейшем.\"\n",
    "\n",
    "https://pymorphy2.readthedocs.io/en/latest/user/guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "def delete_stop_words_with_lemmatizing(words, morph, stop_words):  \n",
    "    default_list = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            lem_word = morph.parse(str(word))[0].normal_form\n",
    "            default_list.append(lem_word)\n",
    "    return list(default_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 of 7\n",
      "Processed 2000 of 6\n",
      "Processed 3000 of 15\n",
      "Processed 4000 of 9\n",
      "Processed 5000 of 14\n",
      "Processed 6000 of 14\n",
      "Processed 7000 of 7\n",
      "Processed 8000 of 16\n",
      "Processed 9000 of 19\n",
      "Processed 10000 of 11\n",
      "Processed 11000 of 7\n",
      "Processed 12000 of 6\n",
      "Processed 13000 of 9\n",
      "Processed 14000 of 8\n",
      "Processed 15000 of 13\n",
      "Processed 16000 of 8\n",
      "Processed 17000 of 6\n",
      "Processed 18000 of 11\n",
      "Processed 19000 of 9\n",
      "Processed 20000 of 10\n",
      "Processed 21000 of 11\n",
      "Processed 22000 of 2\n",
      "Processed 23000 of 7\n",
      "Processed 24000 of 12\n",
      "Processed 25000 of 7\n",
      "Processed 26000 of 11\n",
      "Processed 27000 of 10\n",
      "Processed 28000 of 5\n",
      "Processed 29000 of 13\n",
      "Processed 30000 of 11\n",
      "Processed 31000 of 16\n",
      "Processed 32000 of 3\n",
      "Processed 33000 of 5\n",
      "Processed 34000 of 7\n",
      "Processed 35000 of 17\n",
      "Processed 36000 of 14\n",
      "Processed 37000 of 6\n",
      "Processed 38000 of 9\n",
      "Processed 39000 of 16\n",
      "Processed 40000 of 5\n",
      "Processed 41000 of 6\n",
      "Processed 42000 of 7\n",
      "Processed 43000 of 6\n",
      "Processed 44000 of 7\n",
      "Processed 45000 of 9\n",
      "Processed 46000 of 13\n",
      "Processed 47000 of 9\n",
      "Processed 48000 of 7\n",
      "Processed 49000 of 19\n",
      "Processed 50000 of 9\n",
      "Processed 51000 of 8\n",
      "Processed 52000 of 9\n",
      "Processed 53000 of 3\n",
      "Processed 54000 of 18\n",
      "Processed 55000 of 12\n",
      "Processed 56000 of 6\n",
      "Processed 57000 of 21\n",
      "Processed 58000 of 18\n",
      "Processed 59000 of 7\n",
      "Processed 60000 of 9\n",
      "Processed 61000 of 18\n",
      "Processed 62000 of 10\n",
      "Processed 63000 of 7\n",
      "Processed 64000 of 7\n",
      "Processed 65000 of 7\n",
      "Processed 66000 of 11\n",
      "Processed 67000 of 7\n",
      "Processed 68000 of 7\n",
      "Processed 69000 of 4\n",
      "Processed 70000 of 4\n",
      "Processed 71000 of 10\n",
      "Processed 72000 of 14\n",
      "Processed 73000 of 9\n",
      "Processed 74000 of 11\n",
      "Processed 75000 of 10\n",
      "Processed 76000 of 6\n",
      "Processed 77000 of 6\n",
      "Processed 78000 of 13\n",
      "Processed 79000 of 11\n",
      "Processed 80000 of 7\n",
      "Processed 81000 of 18\n",
      "Processed 82000 of 14\n",
      "Processed 83000 of 4\n",
      "Processed 84000 of 16\n",
      "Processed 85000 of 7\n",
      "Processed 86000 of 5\n",
      "Processed 87000 of 6\n",
      "Processed 88000 of 8\n",
      "Processed 89000 of 17\n",
      "Processed 90000 of 18\n",
      "Processed 91000 of 12\n",
      "Processed 92000 of 8\n",
      "Processed 93000 of 3\n",
      "Processed 94000 of 2\n",
      "Processed 95000 of 16\n",
      "Processed 96000 of 15\n",
      "Processed 97000 of 10\n",
      "Processed 98000 of 9\n",
      "Processed 99000 of 9\n",
      "Processed 100000 of 12\n",
      "Processed 101000 of 14\n",
      "Processed 102000 of 20\n",
      "Processed 103000 of 8\n",
      "Processed 104000 of 6\n",
      "Processed 105000 of 9\n",
      "Processed 106000 of 10\n",
      "Processed 107000 of 6\n",
      "Processed 108000 of 17\n",
      "Processed 109000 of 20\n",
      "Processed 110000 of 8\n",
      "Processed 111000 of 11\n",
      "Processed 112000 of 7\n",
      "Processed 113000 of 8\n",
      "Processed 114000 of 22\n",
      "Processed 115000 of 6\n",
      "Processed 116000 of 7\n",
      "Processed 117000 of 11\n",
      "Processed 118000 of 17\n",
      "Processed 119000 of 9\n",
      "Processed 120000 of 11\n",
      "Processed 121000 of 8\n",
      "Processed 122000 of 13\n",
      "Processed 123000 of 9\n",
      "Processed 124000 of 7\n",
      "Processed 125000 of 6\n",
      "Processed 126000 of 9\n",
      "Processed 127000 of 6\n",
      "Processed 128000 of 4\n",
      "Processed 129000 of 20\n",
      "Processed 130000 of 11\n",
      "Processed 131000 of 4\n",
      "Processed 132000 of 16\n",
      "Processed 133000 of 17\n",
      "Processed 134000 of 8\n",
      "Processed 135000 of 1\n",
      "Processed 136000 of 20\n",
      "Processed 137000 of 5\n",
      "Processed 138000 of 18\n",
      "Processed 139000 of 13\n",
      "Processed 140000 of 10\n",
      "Processed 141000 of 5\n",
      "Processed 142000 of 10\n",
      "Processed 143000 of 4\n",
      "Processed 144000 of 7\n",
      "Processed 145000 of 7\n",
      "Processed 146000 of 11\n",
      "Processed 147000 of 6\n",
      "Processed 148000 of 9\n",
      "Processed 149000 of 18\n",
      "Processed 150000 of 8\n",
      "Processed 151000 of 8\n",
      "Processed 152000 of 5\n",
      "Processed 153000 of 11\n",
      "Processed 154000 of 11\n",
      "Processed 155000 of 8\n",
      "Processed 156000 of 6\n",
      "Processed 157000 of 12\n",
      "Processed 158000 of 7\n",
      "Processed 159000 of 5\n",
      "Processed 160000 of 14\n",
      "Processed 161000 of 5\n",
      "Processed 162000 of 7\n",
      "Processed 163000 of 14\n",
      "Processed 164000 of 16\n",
      "Processed 165000 of 7\n",
      "Processed 166000 of 6\n",
      "Processed 167000 of 7\n",
      "Processed 168000 of 11\n",
      "Processed 169000 of 11\n",
      "Processed 170000 of 10\n",
      "Processed 171000 of 9\n",
      "Processed 172000 of 8\n",
      "Processed 173000 of 5\n",
      "Processed 174000 of 6\n",
      "Processed 175000 of 17\n",
      "Processed 176000 of 13\n",
      "Processed 177000 of 14\n",
      "Processed 178000 of 4\n",
      "Processed 179000 of 6\n",
      "Processed 180000 of 12\n",
      "Processed 181000 of 10\n",
      "Processed 182000 of 5\n",
      "Processed 183000 of 12\n",
      "Processed 184000 of 9\n",
      "Processed 185000 of 7\n",
      "Processed 186000 of 4\n",
      "Processed 187000 of 5\n",
      "Processed 188000 of 10\n",
      "Processed 189000 of 11\n",
      "Processed 190000 of 11\n",
      "Processed 191000 of 3\n",
      "Processed 192000 of 16\n",
      "Processed 193000 of 12\n",
      "Processed 194000 of 2\n",
      "Processed 195000 of 11\n",
      "Processed 196000 of 14\n",
      "Processed 197000 of 14\n",
      "Processed 198000 of 3\n",
      "Processed 199000 of 7\n",
      "Processed 200000 of 17\n",
      "Processed 201000 of 14\n",
      "Processed 202000 of 8\n",
      "Processed 203000 of 13\n",
      "Processed 204000 of 12\n",
      "Processed 205000 of 17\n",
      "Processed 206000 of 10\n",
      "Processed 207000 of 10\n",
      "Processed 208000 of 7\n",
      "Processed 209000 of 5\n",
      "Processed 210000 of 19\n",
      "Processed 211000 of 9\n",
      "Processed 212000 of 7\n",
      "Processed 213000 of 16\n",
      "Processed 214000 of 9\n",
      "Processed 215000 of 13\n",
      "Processed 216000 of 15\n",
      "Processed 217000 of 6\n",
      "Processed 218000 of 10\n",
      "Processed 219000 of 8\n",
      "Processed 220000 of 6\n",
      "Processed 221000 of 12\n",
      "Processed 222000 of 5\n",
      "Processed 223000 of 5\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = []\n",
    "counter = 0\n",
    "for part_of_list in tokenized_words:\n",
    "    lemmatized_words.append(delete_stop_words_with_lemmatizing(part_of_list, morph, stop_words))   \n",
    "    counter += 1\n",
    "    if counter%1000 == 0:\n",
    "        print(\"Processed {} of {}\".format(counter, len(part_of_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223846"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = to_data_frame(lemmatized_words, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>школотый поверь самый общество профилировать п...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>таки немного похожий мальчик равно</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>идиотка испугаться</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>угол сидеть погибать голод порция взять хотя ж...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>значит страшилка блин посмотреть часть создать...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223841</th>\n",
       "      <td>каждый хотеть исправлять</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223842</th>\n",
       "      <td>скучать вправлять мозг равно скучать</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223843</th>\n",
       "      <td>школа говно это идти</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223844</th>\n",
       "      <td>тауриэль грусть обнять</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223845</th>\n",
       "      <td>такси везти работа раздумывать приплатить втащ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comments  sentiment\n",
       "0       школотый поверь самый общество профилировать п...          1\n",
       "1                      таки немного похожий мальчик равно          1\n",
       "2                                      идиотка испугаться          1\n",
       "3       угол сидеть погибать голод порция взять хотя ж...          1\n",
       "4       значит страшилка блин посмотреть часть создать...          1\n",
       "...                                                   ...        ...\n",
       "223841                           каждый хотеть исправлять          0\n",
       "223842               скучать вправлять мозг равно скучать          0\n",
       "223843                               школа говно это идти          0\n",
       "223844                             тауриэль грусть обнять          0\n",
       "223845  такси везти работа раздумывать приплатить втащ...          0\n",
       "\n",
       "[223846 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding (Векторное представление слов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем векторизацию текста 4-мя способами BoW, TF-IDF, Word2Vec, doc2vec.\n",
    "- На этих данных обучим модели и сравним результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words (BoW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lemmatized_words['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['аа', 'ааа', 'аааа', 'ааааа', 'аааааа', 'ааааааа', 'аааааааа', 'ааааааааа', 'аааааааааа', 'ааааааааааа', 'аааааааааааа', 'ааааааааааааа', 'аааааааааааааа', 'ааааааааааааааа', 'аааааааааааааааа', 'ааааааааааааааааа', 'аааааааааааааааааа', 'ааааааааааааааааааа', 'аааааааааааааааааааа', 'ааааааааааааааааааааа', 'аааааааааааааааааааааа', 'ааааааааааааааааааааааа', 'аааааааааааааааааааааааа', 'ааааааааааааааааааааааааа', 'аааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааахахахахахахахахаххахаххахахах', 'аааааааааать', 'ааааааааать', 'аааааааад', 'ааааааааррррра', 'ааааааий', 'ааааааррррра', 'ааааааххха', 'ааааадееть', 'аааааееееееий', 'ааааайщий', 'аааааллиловать', 'ааааам', 'ааааапасность', 'ааааатыдыщ', 'ааааать', 'аааааууыыыааа', 'ааааахахахах', 'ааааахахаххаахах', 'ааааахуенноооо', 'ааааахууууууеееееть', 'аааад', 'ааааеаеий', 'ааааеееий', 'аааалиловать', 'аааам', 'ааааня', 'аааас', 'аааать', 'ааааууууа', 'аааафигеть', 'аааахаахазщха', 'аааахааэаэхахаэахах', 'аааахах', 'аааахи', 'аааахиренный', 'аааахууууууууууууенно', 'ааааххаах', 'ааааххах', 'аааащий', 'аааввва', 'ааад', 'ааадайтесилпопростиьунегопрощение', 'аааеееий', 'аааееий', 'аааж', 'ааанечка', 'аааня', 'аааооо', 'ааап', 'ааар', 'ааарг', 'ааармотавва', 'ааарра', 'ааастыыыыыый', 'ааать', 'ааах', 'ааахаахахаааа', 'ааахааххааахахха', 'ааахах', 'ааахахах', 'ааахахахахахахахахха']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223846, 94121)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 157. GiB for an array with shape (223846, 94121) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-bfc4d99d5de2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 157. GiB for an array with shape (223846, 94121) and data type int64"
     ]
    }
   ],
   "source": [
    "X_arr = X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Word with n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(lemmatized_words['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['аа аа', 'аа ааа', 'аа айн', 'аа акаарыбын', 'аа ап', 'аа арай', 'аа ардчилал', 'аа ахи', 'аа бантик', 'аа бедный', 'аа бифри', 'аа бла', 'аа блин', 'аа бля', 'аа блянулааан', 'аа бог', 'аа болеть', 'аа болзоон', 'аа брат', 'аа быть', 'аа верить', 'аа вон', 'аа вообще', 'аа воот', 'аа вспомнить', 'аа вчера', 'аа вще', 'аа выспаться', 'аа выходить', 'аа говорить', 'аа год', 'аа горло', 'аа господь', 'аа гэж', 'аа даа', 'аа давать', 'аа делать', 'аа день', 'аа дианыч', 'аа долго', 'аа думать', 'аа жарко', 'аа ждать', 'аа заболеть', 'аа забыть', 'аа завтра', 'аа заебалиия', 'аа закончиться', 'аа замёрзлый', 'аа запутаться', 'аа зараз', 'аа заставить', 'аа зато', 'аа збс', 'аа значит', 'аа извинить', 'аа ита', 'аа каждый', 'аа казаться', 'аа капец', 'аа каток', 'аа киев', 'аа класс', 'аа ключ', 'аа код', 'аа красота', 'аа крилла', 'аа ладный', 'аа лан', 'аа любить', 'аа маленький', 'аа мочь', 'аа наверно', 'аа нада', 'аа наивный', 'аа народ', 'аа начать', 'аа начаться', 'аа недавно', 'аа недоангина', 'аа ничего', 'аа новый', 'аа нравиться', 'аа нужно', 'аа обидеть', 'аа один', 'аа первый', 'аа песня', 'аа плиззззза', 'аа плохо', 'аа подумать', 'аа позвать', 'аа пока', 'аа пол', 'аа понимать', 'аа понятно', 'аа понять', 'аа пора', 'аа поржать', 'аа порываться']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer2.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223846, 781179)\n"
     ]
    }
   ],
   "source": [
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.27 TiB for an array with shape (223846, 781179) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-038130915189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX2_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.27 TiB for an array with shape (223846, 781179) and data type int64"
     ]
    }
   ],
   "source": [
    "X2_arr = X2.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Вывод: \n",
    "  - Потеря порядка слов;\n",
    "  - Полученные вектора, не являются информативными и занимают очень много места в памяти;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf_vect.fit_transform(lemmatized_words['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['аа', 'ааа', 'аааа', 'ааааа', 'аааааа', 'ааааааа', 'аааааааа', 'ааааааааа', 'аааааааааа', 'ааааааааааа', 'аааааааааааа', 'ааааааааааааа', 'аааааааааааааа', 'ааааааааааааааа', 'аааааааааааааааа', 'ааааааааааааааааа', 'аааааааааааааааааа', 'ааааааааааааааааааа', 'аааааааааааааааааааа', 'ааааааааааааааааааааа', 'аааааааааааааааааааааа', 'ааааааааааааааааааааааа', 'аааааааааааааааааааааааа', 'ааааааааааааааааааааааааа', 'аааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'аааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааааа', 'ааааааааааааахахахахахахахахаххахаххахахах', 'аааааааааать', 'ааааааааать', 'аааааааад', 'ааааааааррррра', 'ааааааий', 'ааааааррррра', 'ааааааххха', 'ааааадееть', 'аааааееееееий', 'ааааайщий', 'аааааллиловать', 'ааааам', 'ааааапасность', 'ааааатыдыщ', 'ааааать', 'аааааууыыыааа', 'ааааахахахах', 'ааааахахаххаахах', 'ааааахуенноооо', 'ааааахууууууеееееть', 'аааад', 'ааааеаеий', 'ааааеееий', 'аааалиловать', 'аааам', 'ааааня', 'аааас', 'аааать', 'ааааууууа', 'аааафигеть', 'аааахаахазщха', 'аааахааэаэхахаэахах', 'аааахах', 'аааахи', 'аааахиренный', 'аааахууууууууууууенно', 'ааааххаах', 'ааааххах', 'аааащий', 'аааввва', 'ааад', 'ааадайтесилпопростиьунегопрощение', 'аааеееий', 'аааееий', 'аааж', 'ааанечка', 'аааня', 'аааооо', 'ааап', 'ааар', 'ааарг', 'ааармотавва', 'ааарра', 'ааастыыыыыый', 'ааать', 'ааах', 'ааахаахахаааа', 'ааахааххааахахха', 'ааахах', 'ааахахах', 'ааахахахахахахахахха']\n"
     ]
    }
   ],
   "source": [
    "print(tf_vect.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223846, 94121)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
